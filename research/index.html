---
layout: default
title: Research | Shashank Srikant
research: active
description: Research | Shashank Srikant
---
<div class="row vertical-center about-desc">
	<div class="col-sm-6">
		<p class="lead" align="center"><h2>Research</h2></p>
	</div>
	<div class="col-sm-6">
		<p class="lead" align="right"><h4>Full list on <a href="https://scholar.google.com/citations?user=GiVsUSMAAAAJ&hl=en" target="_blank">Google scholar</a></h4></p>
	</div>
</div>	

<div class="newline"></div>

<div class="row vertical-center about-desc">
	<div class="col-sm-12">
	  <h4>Automatic grading of computer programs</h4>
	  <h5>Work done with Varun Aggarwal and Gursimran Singh.</h5>
	  <h5>Published at: KDD 2014 <a href="http://dl.acm.org/citation.cfm?id=2623377" target="_blank">[link]</a>, KDD 2016 <a href="http://dl.acm.org/citation.cfm?id=2939696" target="_blank">[link]</a></h5>
	  <h5>
	  <a href="http://research.aspiringminds.com/question-independent-grading-kdd/" target="_blank">[Blog post]</a>
	  &nbsp;&nbsp;
	  <a href="http://spectrum.ieee.org/at-work/education/moocs-teach-ai-to-grade-computer-programs-as-a-human-would" target="_blank">[Report on IEEE spectrum]</a>
	  </h5>
		   <p style="text-align:justify;">
		   Extant program assessment systems score mostly based on the number of test-cases passed, providing no insight into the competency of the programmer. In addition to grading a program on its programming practices and complexity, the key kernel of the system we designed is a machine-learning based algorithm which determines closeness of the logic of the given program to a correct program. This algorithm uses a set of highly-informative features, derived from the abstract representations of a given program, that capture the program's functionality. These features are then used to learn a model to grade the programs, which are built against evaluations done by experts.
		   </p>
		   <p style="text-align:justify;">
		   Having solved this problem, we found ourselves training models for every programming question we designed, which was a time/human expensive effort. This impedes scaling such a system to new content. We address this issue by presenting a method to grade computer programs which requires no labeled samples for grading responses to a new, unseen question. We extend our previous work wherein we introduced a grammar of features to learn question specific models. In this work, we propose a method to transform those features into a set of features that maintain their structural relation with the labels across questions. Using these features we learn one supervised model across questions, which can then be applied to an ungraded response to an unseen question.
		   </p>
	</div>	   
</div>
<div class="row vertical-center about-desc">
	<div class="col-sm-6">
		   <p align="center"><img src='/img/kdd/kdd_1.png' width = '300' /></p>
	</div>
	<div class="col-sm-6">
		   <p align="center"><iframe width="300" height="200" src="https://www.youtube.com/embed/vMlZwQZMwDs" frameborder="0" allowfullscreen></iframe></p>
	</div>
</div>

<div class="newline"></div>

<div class="row vertical-center about-desc">
	<div class="col-sm-12">
	  <h4>Analyzing test case statistics</h4>
	  <h5>Work done with Varun Aggarwal. Used internally in the group as a tool</h5>
	  <h5><a href="http://research.aspiringminds.com/test-case-analytics/" target="_blank">[Blog post]</a></h5>
		   <p style="text-align:justify;">
		   Test cases evaluate whether a computer program is doing what it’s supposed to do. There are various ways to generate them – automatically based on specifications, say by ensuring code coverage or by subject matter experts (SMEs) who think through conditions based on the problem specification. We asked ourselves whether there was something we could learn by looking at how student programs responded to test cases. Could this help us design better test cases or find flaws in them? By looking at such responses from a data-driven perspective, we wanted to know whether we could .a. design better test cases .b. understand whether there existed any clusters in the way responses on test cases were obtained and .c.  whether we could discover salient concepts needed to solve a particular programming problem, which would then inform us of the right pedagogical interventions. We built a cool tool which helped us look at statistics on over 2500 test cases spread across over fifty programming problems attempted by nearly 18,000 students and job-seekers in a span of four weeks!
		   </p>
	</div>	   
</div>
<div class="row vertical-center about-desc">
	<div class="col-sm-4"> <p align="center"><img src='/img/tc/tc_1.png' width = '300' /></p> </div>
	<div class="col-sm-4"> <p align="center"><img src='/img/tc/tc_5.png' width = '300' /></p> </div>
	<div class="col-sm-4"> <p align="center"><img src='/img/tc/tc_3.png' width = '300' /></p> </div>
</div>
<div class="newline"></div>

<div class="row vertical-center about-desc">
	<div class="col-sm-12">
	  <h4>Learning models for job selection</h4>
	  <h5>Work done with Vinay Shashidhar and Varun Aggarwal</h5>
	  <h5>Published at: Workshop on ML for education, ICML 2015</h5>
	  <h5><a href="http://research.aspiringminds.com/who-is-prepared-to-learn-computer-programming/" target="_blank">[Blog post]</a></h5>
	  We addressed the problem of desgning classifiers to match skills (as measured by our standardized tests) to the right job-roles. The models needed to follow some of these constraints - 
	  <ul>
	  <li>They needed to be theoretically plausible. For instance, a candidate with a higher test score cannot be rejected while accepting one with a lower score. We identified coordinate-wise monotonicity as the weakest structure needed for this to happen.</li>
	  <li>They needed to be simple and human interpretable. This is important because we cannot completely depend on data given its non-causal nature and the sample bias it may contain.</li>
	  <li>It needed to provide a suite of trade-off models with different type-1 and type-2 errors, allowing HR personnel to accommodate prevalent market conditions and allowing them to suit their companies' standards.</li>
	  </ul>
	  Our formal experimental setup answered the following questions - 
	  <ul>
		<li>Could we build employability benchmarks with acceptable type-1 and type-2 errors using our techniques? If organizations use our benchmark for hiring, will they be able to reduce hiring unsatisfactory employees without adverse selection?</li>
		<li>Does an ensemble of linear models provide better prediction accuracy than a single, linear classification model?</li>
		<li>What insight and knowledge discovery may happen by studying our models? Could we discover what combination of parameters and variables determine employability for a job sector?</li>
	  </ul>
	</div>	   
</div>
<div class="row vertical-center about-desc">
	<div class="col-sm-3"><p align="center"><img src='/img/icml/icml2.png' width = '400' /></p> </div>
	<div class="col-sm-3"></div>
	<div class="col-sm-4"> <p align="center"><img src='/img/icml/icml1.png' width = '360' /></p> </div>
</div>

<a name="nips"></a>
<div class="newline"></div>

<div class="row vertical-center about-desc">
	<div class="col-sm-12">
	  <h4>A framework to apply machine learning to subjective assessment tasks</h4>
	  <h5>Work done with Varun Aggarwal and Vinay Shashidhar</h5>
	  <h5>Published at: Workshop on data driven education, NIPS 2012</h5>
	  <h5><a href="/img/papers/nips2013.pdf" target="_blank">[PDF]</a>
	  <a href="http://research.aspiringminds.com/wp-content/uploads/2015/11/Assess-white-paper.pdf" target="_blank">[Related whitepaper]</a>
	  </h5>
	  This is a neat position paper which describes the broader framework of grading open-ended assessments using machine learning. This gives an idea of how one should pick out of a problem in open assessments and use it in casting a problem in ML. This work highlights how solving open-ended assessment tasks generally entail some very hard problems which have no good answers, such as - 
	  <ul>
			<li><b>Getting labeled data</b> - More complex the domain, the harder, more time consuming and more noisy this is going to get. There have been clever workarounds which we demonstrated in areas like speech processing and computer programming. Nevertheless, this is something to reckon with.</li> 
			<li><b>Low data</b> - Say goodbye to big data and all that schmooze. Getting large samples of labeled data will usually be impractical, resulting in small labeled data-sets to work with. This work will generally require applying and evaluating all your fundamental knowledge in ML like regularization, parameter tuning etc. Be prepared.</li>
			<li><b>Sparse data</b> - Things couldn't get more challenging. Say hello to sparse data. Features generated in these problems typically will exceed the number of labeled data points you have. This is very similar to problems in the domain of biology and genetics. Again, no good answers; first principles' approach of penalizing your models right.</li>
	  </ul>
	  Also check out the white-paper <a href="http://research.aspiringminds.com/wp-content/uploads/2015/11/Assess-white-paper.pdf" target="_blank">[link]</a> we came out with which discusses the state of the art in the data-driven assessments space. This was released as part of <a href="http://www.aspiringminds.com/pages/assess/2014/" target="_blank">ASSESS 2014</a>.
	</div>	   
</div>

<div class="newline"></div>

<a name="dskids"></a>
<div class="row vertical-center about-desc">
	<div class="col-sm-12">
	  <h4>Introducing data science to kids</h4>
	  <h5>Work done with Varun Aggarwal</h5>
	  <h5>Submitted to: SIGCSE 2016</h5>
	  <h5><a href="http://www.datasciencekids.org/2015/06/why-data-science-camp.html" target="_blank">[My opinion piece]</a></h5>
	  This was a fun project from the word go. We came up with a carefully designed tutorial to teach kids the basics of data science. Involved hands-on exercises designed on MS Excel. We also designed a decision-tree based super simple classifier which could provide a measure for classifier-accuracy.
	  <br/>
	  Led to co-founding <a href="http://datasciencekids.org">Data science for kids</a>. Check it out pronto!
	</div>	   
</div>

<div class="newline"></div>

<div class="row vertical-center about-desc">
	<div class="col-sm-12">
	  <h4>Partially adaptive tests</h4>
	  <h5>Work done with Varun Aggarwal. Used internally in the group as a tool</h5>
	  [[Project details]]
	</div>
</div>

<div class="newline"></div>

<div class="row vertical-center about-desc">
	<div class="col-sm-12">
	  <h4>Predicting web-surfers' interests from their interaction with web browsers</h4>
	  <h5>Work done with Kunal Sangwan, Sohil Arora and Dr. Jitender Chhabra.</h5>
	  <h5><a href="/img/undergrad_reports/btp2.pdf" target="_blank">[Report] </a></h5>
	  [[Probject details]]
	</div>
</div>

<div class="newline"></div>

<div class="row vertical-center about-desc">
	<div class="col-sm-12">
	  <h4>Predicting user movement in public transport networks</h4>
	  <h5>Work done with Kunal Sangwan, Sohil Arora and Dr. Jitender Chhabra.</h5>
	  <h5>Submitted to a student olympiad organized at ICSE 2011</h5>
	  <h5><a href="/img/undergrad_reports/btp1.pdf" target="_blank">[Report] </a>
	  &nbsp;&nbsp;
	  <a href="/img/undergrad_reports/btp1_slides.pdf" target="_blank">[Slides] </a></h5>
	  [[Project details]]
	</div>
</div>

<div class="newline"></div>

<div class="row vertical-center about-desc">
	<div class="col-sm-12">
	  <h4>Parallelization of weighted sequence comparison using EBWT</h4>
	  <h5>Work done with Binay Pandey and Dr. Rajdeep Niyogi.</h5>
	  <h5><a href="/img/undergrad_reports/roorkee.pdf" target="_blank">[Report]</a></h5>
	  [[Project details]]
	</div>
</div>

<div class="newline"></div>