---
layout: default
title: shashank-srikant.github.io
description: Personal webpage of Shashank Srikant
---
<div class="row vertical-center about-desc">
	<div class="col-sm-6">
		<p class="lead" align="center"><h2>Research</h2></p>
	</div>
	<div class="col-sm-6">
		<p class="lead" align="right"><h4>Full list on <a href="https://scholar.google.com/citations?user=GiVsUSMAAAAJ&hl=en" target="_blank">Google scholar</a></h4></p>
	</div>
</div>	

<div class="newline"></div>

<div class="row vertical-center about-desc">
	<div class="col-sm-12">
	  <h4>Automatic grading of computer programs</h4>
	  <br/>
	  <h4>Published at: KDD 2014 and KDD 2016</h4>
		   <p style="text-align:justify;">
		   Extant program assessment systems score mostly based on the number of test-cases passed, providing no insight into the competency of the programmer. In addition to grading a program on its programming practices and complexity, the key kernel of the system we designed is a machine-learning based algorithm which determines closeness of the logic of the given program to a correct program. This algorithm uses a set of highly-informative features, derived from the abstract representations of a given program, that capture the program's functionality. These features are then used to learn a model to grade the programs, which are built against evaluations done by experts.
		   </p>
		   <p style="text-align:justify;">
		   Having solved this problem, we found ourselves training models not only for every programming question we designed, which was a time/human expensive effort. The availability and time taken by experts to create a labeled set of programs for each question is a major bottleneck in scaling such a system. We address this issue by presenting a method to grade computer programs which requires no labeled samples for grading responses to a new, unseen question. We extend our previous work wherein we introduced a grammar of features to learn question specific models. In this work, we propose a method to transform those features into a set of features that maintain their structural relation with the labels across questions. Using these features we learn one supervised model across questions, which can then be applied to an ungraded response to an unseen question.
		   </p>
	</div>	   
</div>
<div class="row vertical-center about-desc">
	<div class="col-sm-4">
		   <p align="center"><iframe width="200" height="100" src="https://www.youtube.com/embed/vMlZwQZMwDs" frameborder="0" allowfullscreen></iframe></p>
	</div>
	<div class="col-sm-4">
		   <p align="center"><img src='img/kdd_2.png' width = '150' /></p>
	</div>
	<div class="col-sm-4">
		   <p align="center"><iframe width="200" height="100" src="https://www.youtube.com/embed/vMlZwQZMwDs" frameborder="0" allowfullscreen></iframe></p>
	</div>
</div>


<div class="newline"></div>

<div class="row vertical-center about-desc">
	<div class="col-sm-12">
	  <h4>Question independent grading using machine learning: The case of computer program grading</h4>
		   <p style="text-align:justify;">
		   Learning supervised models to grade open-ended responses is an expensive process. A model has to be trained for every prompt/question separately, which in turn requires graded samples. In automatic programming evaluation specically, the focus of this work, this issue is amplified. The models have to be trained not only for every question but also for every language the question is offered in. Moreover, the availability and time taken by experts to create a labeled set of programs for each question is a major bottleneck in scaling such a system. We address this issue by presenting a method to grade computer programs which requires no labeled samples for grading responses to a new, unseen question. We extend our previous work wherein we introduced a grammar of features to learn question specific models. In this work, we propose a method to transform those features into a set of features that maintain their structural relation with the labels across questions. Using these features we learn one supervised model across questions, which can then be applied to an ungraded response to an unseen question. We show that our method rivals the performance of both, question specific models and the consensus among human experts while substantially outperforming extant ways of evaluating codes. The learning from this work is transferable to other grading tasks such as math question grading and also provides a new variation to the supervised learning approach.
		   </p>
		   <p><strong>Publications:</strong> KDD 2016, Plenary session presentation.</p> 
	</div>
</div>